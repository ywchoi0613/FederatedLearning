{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl9dbweQXujC",
        "outputId": "4e647c87-daf8-4249-cec7-4de84c57a6ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 101031097.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 54199415.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31523275.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1178140.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Client 1: {2: 600, 6: 600}\n",
            "Client 2: {9: 1200}\n",
            "Client 3: {6: 600, 7: 600}\n",
            "Client 4: {7: 1200}\n",
            "Client 5: {2: 1200}\n",
            "Client 6: {0: 600, 2: 600}\n",
            "Client 7: {4: 600, 5: 600}\n",
            "Client 8: {2: 600, 6: 600}\n",
            "Client 9: {8: 600, 4: 600}\n",
            "Client 10: {1: 600, 4: 600}\n",
            "Client 11: {1: 600, 3: 600}\n",
            "Client 12: {8: 600, 7: 600}\n",
            "Client 13: {0: 600, 5: 600}\n",
            "Client 14: {9: 600, 6: 600}\n",
            "Client 15: {8: 600, 4: 600}\n",
            "Client 16: {0: 600, 7: 600}\n",
            "Client 17: {9: 600, 5: 600}\n",
            "Client 18: {0: 1200}\n",
            "Client 19: {4: 1200}\n",
            "Client 20: {2: 1200}\n",
            "Client 21: {1: 600, 5: 600}\n",
            "Client 22: {8: 600, 1: 600}\n",
            "Client 23: {4: 600, 6: 600}\n",
            "Client 24: {8: 600, 9: 600}\n",
            "Client 25: {8: 1200}\n",
            "Client 26: {0: 600, 7: 600}\n",
            "Client 27: {3: 600, 7: 600}\n",
            "Client 28: {8: 1200}\n",
            "Client 29: {2: 600, 3: 600}\n",
            "Client 30: {1: 600, 7: 600}\n",
            "Client 31: {5: 600, 6: 600}\n",
            "Client 32: {0: 600, 4: 600}\n",
            "Client 33: {1: 600, 5: 600}\n",
            "Client 34: {9: 600, 1: 600}\n",
            "Client 35: {0: 600, 7: 600}\n",
            "Client 36: {9: 600, 3: 600}\n",
            "Client 37: {0: 600, 5: 600}\n",
            "Client 38: {1: 1200}\n",
            "Client 39: {0: 600, 5: 600}\n",
            "Client 40: {8: 600, 9: 600}\n",
            "Client 41: {4: 600, 6: 600}\n",
            "Client 42: {6: 600, 7: 600}\n",
            "Client 43: {3: 1200}\n",
            "Client 44: {2: 600, 5: 600}\n",
            "Client 45: {3: 600, 6: 600}\n",
            "Client 46: {9: 600, 1: 600}\n",
            "Client 47: {2: 600, 3: 600}\n",
            "Client 48: {9: 600, 5: 600}\n",
            "Client 49: {3: 600, 6: 600}\n",
            "Client 50: {3: 600, 4: 600}\n",
            "Global Round 1/120\n",
            "Global Round 2/120\n",
            "Global Round 3/120\n",
            "Global Round 4/120\n",
            "Global Round 5/120\n",
            "Global Round 6/120\n",
            "Global Round 7/120\n",
            "Global Round 8/120\n",
            "Global Round 9/120\n",
            "Global Round 10/120\n",
            "Global Round 11/120\n",
            "Global Round 12/120\n",
            "Global Round 13/120\n",
            "Global Round 14/120\n",
            "Global Round 15/120\n",
            "Global Round 16/120\n",
            "Global Round 17/120\n",
            "Global Round 18/120\n",
            "Global Round 19/120\n",
            "Global Round 20/120\n",
            "Global Round 21/120\n",
            "Global Round 22/120\n",
            "Global Round 23/120\n",
            "Global Round 24/120\n",
            "Global Round 25/120\n",
            "Global Round 26/120\n",
            "Global Round 27/120\n",
            "Global Round 28/120\n",
            "Global Round 29/120\n",
            "Global Round 30/120\n",
            "Global Round 31/120\n",
            "Global Round 32/120\n",
            "Global Round 33/120\n",
            "Global Round 34/120\n",
            "Global Round 35/120\n",
            "Global Round 36/120\n",
            "Global Round 37/120\n",
            "Global Round 38/120\n",
            "Global Round 39/120\n",
            "Global Round 40/120\n",
            "Global Round 41/120\n",
            "Global Round 42/120\n",
            "Global Round 43/120\n",
            "Global Round 44/120\n",
            "Global Round 45/120\n",
            "Global Round 46/120\n",
            "Global Round 47/120\n",
            "Global Round 48/120\n",
            "Global Round 49/120\n",
            "Global Round 50/120\n",
            "Global Round 51/120\n",
            "Global Round 52/120\n",
            "Global Round 53/120\n",
            "Global Round 54/120\n",
            "Global Round 55/120\n",
            "Global Round 56/120\n",
            "Global Round 57/120\n",
            "Global Round 58/120\n",
            "Global Round 59/120\n",
            "Global Round 60/120\n",
            "Global Round 61/120\n",
            "Global Round 62/120\n",
            "Global Round 63/120\n",
            "Global Round 64/120\n",
            "Global Round 65/120\n",
            "Global Round 66/120\n",
            "Global Round 67/120\n",
            "Global Round 68/120\n",
            "Global Round 69/120\n",
            "Global Round 70/120\n",
            "Global Round 71/120\n",
            "Global Round 72/120\n",
            "Global Round 73/120\n",
            "Global Round 74/120\n",
            "Global Round 75/120\n",
            "Global Round 76/120\n",
            "Global Round 77/120\n",
            "Global Round 78/120\n",
            "Global Round 79/120\n",
            "Global Round 80/120\n",
            "Global Round 81/120\n",
            "Global Round 82/120\n",
            "Global Round 83/120\n",
            "Global Round 84/120\n",
            "Global Round 85/120\n",
            "Global Round 86/120\n",
            "Global Round 87/120\n",
            "Global Round 88/120\n",
            "Global Round 89/120\n",
            "Global Round 90/120\n",
            "Global Round 91/120\n",
            "Global Round 92/120\n",
            "Global Round 93/120\n",
            "Global Round 94/120\n",
            "Global Round 95/120\n",
            "Global Round 96/120\n",
            "Global Round 97/120\n",
            "Global Round 98/120\n",
            "Global Round 99/120\n",
            "Global Round 100/120\n",
            "Global Round 101/120\n",
            "Global Round 102/120\n",
            "Global Round 103/120\n",
            "Global Round 104/120\n",
            "Global Round 105/120\n",
            "Global Round 106/120\n",
            "Global Round 107/120\n",
            "Global Round 108/120\n",
            "Global Round 109/120\n",
            "Global Round 110/120\n",
            "Global Round 111/120\n",
            "Global Round 112/120\n",
            "Global Round 113/120\n",
            "Global Round 114/120\n",
            "Global Round 115/120\n",
            "Global Round 116/120\n",
            "Global Round 117/120\n",
            "Global Round 118/120\n",
            "Global Round 119/120\n",
            "Global Round 120/120\n",
            "\n",
            "Evaluating with Eth = 0.4:\n",
            "Client 1 Accuracy: 19.77%\n",
            "Client 1 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 2 Accuracy: 10.09%\n",
            "Client 2 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 3 Accuracy: 19.78%\n",
            "Client 3 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 4 Accuracy: 10.28%\n",
            "Client 4 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 5 Accuracy: 10.32%\n",
            "Client 5 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 6 Accuracy: 19.88%\n",
            "Client 6 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 7 Accuracy: 18.65%\n",
            "Client 7 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 8 Accuracy: 19.78%\n",
            "Client 8 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 9 Accuracy: 19.41%\n",
            "Client 9 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 10 Accuracy: 21.15%\n",
            "Client 10 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 11 Accuracy: 21.39%\n",
            "Client 11 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 12 Accuracy: 19.77%\n",
            "Client 12 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 13 Accuracy: 18.50%\n",
            "Client 13 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 14 Accuracy: 19.60%\n",
            "Client 14 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 15 Accuracy: 19.42%\n",
            "Client 15 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 16 Accuracy: 19.96%\n",
            "Client 16 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 17 Accuracy: 18.77%\n",
            "Client 17 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 18 Accuracy: 9.80%\n",
            "Client 18 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 19 Accuracy: 9.82%\n",
            "Client 19 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 20 Accuracy: 10.32%\n",
            "Client 20 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 21 Accuracy: 20.22%\n",
            "Client 21 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 22 Accuracy: 21.02%\n",
            "Client 22 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 23 Accuracy: 19.25%\n",
            "Client 23 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 24 Accuracy: 19.45%\n",
            "Client 24 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 25 Accuracy: 9.74%\n",
            "Client 25 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 26 Accuracy: 19.96%\n",
            "Client 26 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 27 Accuracy: 20.18%\n",
            "Client 27 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 28 Accuracy: 9.74%\n",
            "Client 28 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 29 Accuracy: 20.06%\n",
            "Client 29 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 30 Accuracy: 21.48%\n",
            "Client 30 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 31 Accuracy: 18.27%\n",
            "Client 31 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 32 Accuracy: 19.54%\n",
            "Client 32 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 33 Accuracy: 20.21%\n",
            "Client 33 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 34 Accuracy: 21.34%\n",
            "Client 34 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 35 Accuracy: 19.99%\n",
            "Client 35 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 36 Accuracy: 19.82%\n",
            "Client 36 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 37 Accuracy: 18.50%\n",
            "Client 37 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 38 Accuracy: 11.35%\n",
            "Client 38 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 39 Accuracy: 18.53%\n",
            "Client 39 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 40 Accuracy: 19.35%\n",
            "Client 40 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 41 Accuracy: 19.22%\n",
            "Client 41 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 42 Accuracy: 19.73%\n",
            "Client 42 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 43 Accuracy: 10.10%\n",
            "Client 43 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 44 Accuracy: 19.08%\n",
            "Client 44 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 45 Accuracy: 19.60%\n",
            "Client 45 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 46 Accuracy: 21.35%\n",
            "Client 46 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 47 Accuracy: 20.12%\n",
            "Client 47 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 48 Accuracy: 18.78%\n",
            "Client 48 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 49 Accuracy: 19.58%\n",
            "Client 49 Decision Ratio: {'client': 1.0, 'server': 0.0}\n",
            "Client 50 Accuracy: 19.81%\n",
            "Client 50 Decision Ratio: {'client': 1.0, 'server': 0.0}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "# Global parameters for the learning process\n",
        "learning_rate = 0.01\n",
        "gamma = 0.5  # Balance between client and server losses\n",
        "lambda_value = 0.2  # Personalization vs. generalization balance\n",
        "num_clients = 50\n",
        "num_global_rounds = 120\n",
        "device = torch.device(\"cpu\")  # Explicitly using CPU for demonstration\n",
        "\n",
        "# MNIST Data loading and preparation\n",
        "def load_mnist_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    num_shards_per_client = 2\n",
        "    shards_idx = np.random.permutation(100)\n",
        "    client_data_idx = [np.concatenate([np.where(np.array(train_dataset.targets) == (s % 10))[0][:600] for s in shards_idx[i*num_shards_per_client:(i+1)*num_shards_per_client]]) for i in range(num_clients)]\n",
        "\n",
        "    client_datasets = [DataLoader(Subset(train_dataset, indices), batch_size=50, shuffle=True) for indices in client_data_idx]\n",
        "\n",
        "    # New part: Printing the label distribution per client\n",
        "    for i, indices in enumerate(client_data_idx):\n",
        "        labels = [int(train_dataset.targets[idx]) for idx in indices]\n",
        "        label_distribution = {label: labels.count(label) for label in set(labels)}\n",
        "        print(f\"Client {i+1}: {label_distribution}\")\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "    return client_datasets, test_loader\n",
        "\n",
        "# Model Definitions\n",
        "class ClientModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientModel, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_layers(x)\n",
        "\n",
        "class ServerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerModel, self).__init__()\n",
        "        self.fc_input_size = 64 * 7 * 7  # Example: Adjust based on ClientModel's output\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_size, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.fc_input_size)  # Reshape the input appropriately\n",
        "        return self.fc_layers(x)\n",
        "\n",
        "class AuxiliaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AuxiliaryClassifier, self).__init__()\n",
        "        self.classifier = nn.Linear(64*7*7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# Loss function definitions\n",
        "def client_side_loss(client_output, labels):\n",
        "    return F.cross_entropy(client_output, labels)\n",
        "\n",
        "def server_side_loss(server_output, labels):\n",
        "    return F.cross_entropy(server_output, labels)\n",
        "\n",
        "# Placeholder for model aggregation logic\n",
        "def aggregate_models(server_model, client_models, alpha, lambda_value):\n",
        "    global_dict = server_model.state_dict()\n",
        "    for k, v in global_dict.items():\n",
        "        weighted_sum = sum((client_models[i].state_dict()[k] - v) * alpha[i] * lambda_value + v for i in range(len(client_models)))\n",
        "        global_dict[k] = weighted_sum / len(client_models)\n",
        "    server_model.load_state_dict(global_dict)\n",
        "    return server_model\n",
        "\n",
        "# New function to perform server-side training and update\n",
        "def train_server_model(server_model, aggregated_outputs, labels, optimizer_server):\n",
        "    optimizer_server.zero_grad()\n",
        "    loss = server_side_loss(aggregated_outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer_server.step()\n",
        "\n",
        "# Training function for a client model\n",
        "def train_client_model(client_model, server_model, auxiliary_classifier, client_data_loader, optimizer_client, optimizer_aux, gamma, lambda_value):\n",
        "    client_model.train()\n",
        "    auxiliary_classifier.train()\n",
        "\n",
        "    for data, target in client_data_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_client.zero_grad()\n",
        "        optimizer_aux.zero_grad()\n",
        "\n",
        "        client_output = client_model(data)\n",
        "        aux_output = auxiliary_classifier(client_output)\n",
        "\n",
        "        # Adjusting the loss to incorporate lambda_value\n",
        "        loss = (gamma * client_side_loss(aux_output, target)) + ((1 - lambda_value) * server_side_loss(server_model(client_output), target))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer_client.step()\n",
        "        optimizer_aux.step()\n",
        "\n",
        "def perform_inference(client_model, server_model, auxiliary_classifier, data, eth):\n",
        "    client_model.eval()\n",
        "    server_model.eval()  # 서버 모델을 평가 모드로 설정\n",
        "    auxiliary_classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        client_output = client_model(data)  # 클라이언트 모델의 출력\n",
        "        probs = F.softmax(auxiliary_classifier(client_output), dim=1)\n",
        "        entropy = -torch.sum(probs * torch.log(probs + 1e-5), dim=1).mean()\n",
        "\n",
        "        if entropy < eth:\n",
        "            # 엔트로피가 임계값보다 낮으면, 클라이언트 모델과 보조 분류기의 출력을 사용\n",
        "            decision = 'client'\n",
        "            output = probs\n",
        "        else:\n",
        "            # 엔트로피가 임계값 이상이면, 서버 모델로 전달하여 추론\n",
        "            decision = 'server'\n",
        "            # 서버 모델에 클라이언트 모델의 출력을 전달\n",
        "            server_output = server_model(client_output.view(client_output.size(0), -1))  # Flatten if necessary\n",
        "            output = F.softmax(server_output, dim=1)\n",
        "\n",
        "        return output, decision\n",
        "\n",
        "def main():\n",
        "    client_datasets, test_loader = load_mnist_data()\n",
        "\n",
        "    client_models = [ClientModel().to(device) for _ in range(num_clients)]\n",
        "    server_model = ServerModel().to(device)\n",
        "    auxiliary_classifiers = [AuxiliaryClassifier().to(device) for _ in range(num_clients)]\n",
        "\n",
        "    optimizer_clients = [optim.SGD(model.parameters(), lr=learning_rate) for model in client_models]\n",
        "    optimizer_auxs = [optim.SGD(aux.parameters(), lr=learning_rate) for aux in auxiliary_classifiers]\n",
        "    optimizer_server = optim.SGD(server_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    #eth_values = [0.05, 0.1, 0.2, 0.4, 0.8, 1.2, 1.6, 2.3]\n",
        "    eth_values = [0.4]\n",
        "\n",
        "    # Simulate global rounds of training\n",
        "    for epoch in range(num_global_rounds):\n",
        "        print(f\"Global Round {epoch+1}/{num_global_rounds}\")\n",
        "        for i, client_data_loader in enumerate(client_datasets):\n",
        "            train_client_model(client_models[i], server_model, auxiliary_classifiers[i], client_data_loader, optimizer_clients[i], optimizer_auxs[i], gamma, lambda_value)\n",
        "\n",
        "    # 각 eth 값에 대해 실행\n",
        "    for eth in eth_values:\n",
        "        print(f\"\\nEvaluating with Eth = {eth}:\")\n",
        "        all_accuracy = []\n",
        "        all_decision_ratios = []\n",
        "\n",
        "        for i, client_data_loader in enumerate(client_datasets):\n",
        "            accuracy, decision_ratio = perform_inference_and_get_accuracy(client_models[i], server_model, auxiliary_classifiers[i], test_loader, eth)\n",
        "            all_accuracy.append(accuracy)\n",
        "            all_decision_ratios.append(decision_ratio)\n",
        "            print(f\"Client {i+1} Accuracy: {accuracy:.2f}%\")\n",
        "            print(f\"Client {i+1} Decision Ratio: {decision_ratio}\")\n",
        "\n",
        "def perform_inference_and_get_accuracy(client_model, server_model, auxiliary_classifier, test_loader, eth):\n",
        "    client_model.eval()\n",
        "    auxiliary_classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    decision_counts = {'client': 0, 'server': 0}  # 클라이언트/서버 결정 카운터 추가\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:  # Evaluating on test data\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output, decision = perform_inference(client_model, server_model, auxiliary_classifier, data, eth)\n",
        "            decision_counts[decision] += 1  # 결정 업데이트\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Use the prediction (either from client or server)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    accuracy = 100. * correct / total if total > 0 else 0\n",
        "    decision_ratio = {key: value / sum(decision_counts.values()) for key, value in decision_counts.items()}  # 비율 계산\n",
        "    return accuracy, decision_ratio  # 비율도 반환\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}